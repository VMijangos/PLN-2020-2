{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 2\n",
    "\n",
    "Procesamiento de Lenguaje Natural\n",
    "Facultad de Ingeniería, UNAM\n",
    "\n",
    "González Flores Andrés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrucciones\n",
    "\n",
    "A partir del corpus proporcionado (\"corpusML.txt\") realizar un modelo del lenguaje neuronal con base en la arquitectura propuesta por Bengio (2003).\n",
    "\n",
    "Síganse los siguientes pasos:\n",
    "\n",
    "1. Limpiar los textos y aplicar stemming a las palabras.\n",
    "2. Insertar símbolos de inicio y final de cadena.\n",
    "3. Obtener los bigramas que aparecen en el texto (indexar numéricamente).\n",
    "4. Entrenar con los bigramas la red neuronal y obtener los valores para los hiperparámetros. Tomar de 100 unidades para la primera capa oculta (capa lineal) y 300 para la segunda capa oculta (capa con tanh).\n",
    "5. Obtener las matrices $A$ y $\\Pi$ a partir de las salidas de la red neuronal (probabilidad Softmax).\n",
    "6. Evaluar el modelo (con Entropía).\n",
    "7. Calcular la probabilidad de las siguientes oraciones:\n",
    "    - Nos bañamos con agua caliente\n",
    "    - El animalito le olía la cabeza\n",
    "    - Pascuala ordeñaba las vacas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desarrollo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar módulos\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk import SnowballStemmer\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "from tqdm.notebook import tqdm as nbtqdm\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de constantes\n",
    "SEED = 420\n",
    "stemmer_esp = SnowballStemmer('spanish')\n",
    "CORPUS_PATH = './Data/corpusML.txt'\n",
    "BOS = '<BOS>'\n",
    "EOS = '<EOS>'\n",
    "UNK = '<unk>'\n",
    "\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 1. Limpieza de corpus\n",
    "\n",
    "Limpiar los textos y aplicar stemming a las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comenc', 'a', 'trabaj', 'y', 'me', 'peg', 'me', 'maltrat', 'con', 'chicot']\n",
      "['mis', 'patron', 'me', 'peg', 'porqu', 'no', 'me', 'quer', 'apur', 'porqu', 'era', 'floj']\n",
      "['por', 'eso', 'me', 'hab', 'peg']\n",
      "['cuand', 'me', 'peg', 'ya', 'entonc', 'me', 'quit']\n",
      "['pues', 'entonc', 'no', 'quis', 'trabaj']\n"
     ]
    }
   ],
   "source": [
    "with open(CORPUS_PATH, 'r', encoding='utf-8') as f:\n",
    "    corpus = [\n",
    "        list(map(lambda token: stemmer_esp.stem(token), # Mapeo cada token a su stem\n",
    "            re.findall('[a-zA-zñáéíóúü]+', linea.lower()) # Devuelve una lista con todas las ocurrencias que coincidan con la regex\n",
    "        ))\n",
    "        for linea in f\n",
    "    ]\n",
    "    for docs in corpus[:5]:\n",
    "        print(docs) # Muestro 5 ejemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divido el corpus en entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tamaño del corpus de entrenamiento: 751\n",
      "Ejemplos:\n",
      "['per', 'pues', 'si', 'el', 'padr', 'del', 'hombr', 'quer', 'lo', 'iba', 'a', 'dej']\n",
      "['hem', 'fabric', 'un', 'temascal', 'par', 'bañ']\n",
      "['ni', 'par', 'el', 'jabon', 'encontr']\n",
      "['mi', 'difunt', 'padrecit', 'fue', 'qui', 'me', 'cur']\n",
      "['se', 'traslad', 'de', 'igual', 'maner', 'y', 'se', 'le', 'mov', 'el', 'pal']\n",
      "\n",
      "Tamaño del corpus de evaluación: 323\n",
      "Ejemplos:\n",
      "  ['y', 'cuart', 'par', 'que', 'vay', 'dos', 'segund', 'palm']\n",
      "  ['le', 'dig', 'pues', 'que', 'no', 'es', 'tu', 'tambor', 'le', 'dig']\n",
      "  ['com', 'tambien', 'se', 'llen', 'de', 'agu']\n",
      "  ['juan', 'se', 'cay', 'del', 'tech']\n",
      "  ['sobr', 'un', 'refresquit', 'y', 'algo', 'de', 'vinit']\n"
     ]
    }
   ],
   "source": [
    "train_corpus, eval_corpus = train_test_split(corpus, test_size=0.3)\n",
    "\n",
    "print(f'\\nTamaño del corpus de entrenamiento: {len(train_corpus)}')\n",
    "print('Ejemplos:')\n",
    "for docs in train_corpus[:5]:\n",
    "        print(docs) # Muestro 5 ejemplos\n",
    "\n",
    "print(f'\\nTamaño del corpus de evaluación: {len(eval_corpus)}')\n",
    "print('Ejemplos:')\n",
    "for docs in eval_corpus[:5]:\n",
    "        print(' ', docs) # Muestro 5 ejemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muestro la curva de zipf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEaCAYAAAD+E0veAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3df5yUdb338deHFXEhXTTQblcQdImCKLc4idF5ZOdOwU5bW2mGWscyPFbW8djNHZ54HLLwSIeOmfcxTcuoNH/etkloaBGZJiUKhh7zVkmRxaOoQAqky/K5/7iuGS5m58c1uzNzzTXzfj4e89iZa675zmfmmpnPfn9c36+5OyIiIgDDkg5ARETqh5KCiIhkKSmIiEiWkoKIiGQpKYiISJaSgoiIZCkpyABmNsHM3Mz2q0LZp5vZnWXsv8jMXjCz/465v5tZx+AjzJZTtfdgKMxslZl9Juk46pmZjTezV8ysJelY0khJoYbM7DQzWxN+YJ81szvM7N1Jx1VJ4WvLvfSZ2QYAd7/O3U+MWdY44EvAFHd/QzXjHgozO97MNiUdR7XUa4IsxN03uvvr3L1/qGU1YxJWUqgRMzsfuBT4N+AwYDzwHeBDgyirbr+c4ZcxewHeCLwELBpEcUcCL7r78xUNUiqunj+TUiZ316XKF6ANeAU4pcg+S4FFkdvHA5sit58Cvgz8EXgVWADcklPGt4HLwuufAh4FXgY2AP9Y5LlbgG8CL4T7fh5wYL9I/N8HngV6CX7gW2K87v2Ae4DvRbadCdwTue3AF8PnfQFYQvDPyvuAXcCe8L1bWuA55oVxbQY+HZbXEd43InxdG4HngCuB1kG+B3nfT2BUTpyvAIeHz31pGNfm8PqI8DFjgJ8D2wgS5m+BYQXiOgH4E7Ad+E/gN8BnIvd/OoxrK7ACOLLI8ZgB/C583oeA4yP3rQK+DtwbvsY7gTHhfRvD9yLz+o4Lj+O9wLfC17CoVDxhGecAj4f3Xw5YeN/RwErgxfAYXAeMzvn8zyP4/O8g+DweBtwRxvtL4OBw3wnE/PyGr+Oe8NhvBf4MnBTedxHQD/w1fN3/GW5/F3B/eEzuB96V9G9MRX+vkg6gGS7AbGB35kNaYJ+llE4K64BxQCvBf9E7gYPC+1vCD/2M8Pbfh180A94T7vv2As99DsEPzzjgEODXOV+qHuC7BD+AhwJ/oEiSiZR7CbAWOCCy7UwGJoVfh887Hvh/hD96ue9Bgff1OeAtYWw/Yd+kcClwW1j2gcAy4OJBvgcF3898cQJfA1aH79dYgh/jr4f3XUyQoIaHl78l/HHMKWMM8Bfg5HC/fw4/R5n3pxt4AngzQQJeAPyuwOtrJ/jBfT9B0j0hvD02vH8V8CRBza41vL04vG9C9L2IHMfdwBfC524tFU9Yxs+B0eGx3gLMDu/rCGMaEb5fdwOX5nz+VxMkgnbgeeBBoDN8zEpgYb54KfL5DV9HHzCX4Dv0WYIkbpH3JZqEDyFIHp8IX+Oc8Pbrk/6dqdjvVdIBNMMFOB347xL7LKV0Uvh0zmPuAT4ZXj8BeLJI+T3APxW4byVwTuT2iZkvVfglfJXIf9jhF+HXJV7PR8Mvy1E5289kYFKYHbn9OeBX+d6DPM9xDeEPV3j7jWF5HQQ/3juAoyP3Hwf8udz3oNT7mS9Ogh/Y90duzwKeCq9/DfgZYfIq8vo+CayO3DZgE3uTwh3AWZH7hxEkqyPzlPVl4Mc521YA/xBeXwUsyDkOvwivT8h9L8LjuDGnvKLxhGW8O3L/TcD8Aq+9G1ib8/k/PXL7/wJXRG5/AejJjbfU5zd8HU9E7hsZPvYNkfclmhQ+AfwhJ9b7gDOLHcs0XdSnUBsvAmMq0O76TM7tnxB8wAFOC28DYGYnmdlqM3vJzLYR/Ic4pkC5h+eU/XTk+pEE/6U+a2bbwrK+S/AfV15mNomgun6mu28o/bIGPPfhMR5TKu6xBF/wByJx/yLcXm5Z5b6fmfKiZURf1xKC/6jvNLMNZjY/Tkwe/AJFYzwS+Hbk9b1EkDja85R1JHBKZt9w/3cD/yOyT3SE107gdUVeHwz8PMaJJ+9zmNmhZnaDmfWa2V+Aaxn4/j4Xub4rz+188cb5/GZjcved4dVCrz33uBLezveep5KSQm3cR9Au2V1knx0EP2IZ+UbbeM7tm4HjzewI4MOEScHMRhD8J/VN4DB3Hw3cTvAFzedZgmaTjPGR688Q/Kc1xt1Hh5eD3H1qvoLMbGT43Fe6+88KPF+u3OfeHPNxxeJ+geCHYmok7jYPOr/LKivG+5l7XAhfw5E55W0GcPeX3f1L7n4U0AWcb2b/s1RMZmY5MT5D0AwyOnJpdfff5SnrGYKaQnTfUe6+OM++ufK9vnzby4kn18VheW9194OAMyj8eS1HWZ/fPHJfY+5xheDY9g4xzrqhpFAD7r4d+FfgcjPrNrORZjY8/O/z38Pd1gHvN7NDzOwNwHkxyt1CUL39AUGzyKPhXfsTtLNuAXab2UkEzSGF3AR80cyOMLODgex/ru7+LEGn43+Y2UFmNszMjjaz9xQo6wqC/xC/Uir+iHlmdnA4BPWfgBtjPu4m4EwzmxImo4WRuPcAVwPfMrNDAcys3cxmFSkr73tA6ffzOeD1ZtYW2XY9sMDMxprZGILjf20YxwfMrCP8kf8LQWdmvuGTy4GpZvaRsJb5Rfb9Z+FK4AIzmxqW22ZmpxR4fdcCXWY2y8xazOyAcCjtEQX2j9pC0JF+VIn9yokn14EEnbnbzKydoFN5yAbx+c31HPu+7tuBN4bDy/czs1OBKQR9JQ1BSaFG3P0S4HyCzrctBP/BnEvQNg3wY4IRIU8RfIjj/jD+hGCkTrbpyN1fJvgBuYmgXf80gg7XQq4maF9+iKDz7tac+z9J8MP4X2F5t7BvswMQnDQU7jsD2J57vkKR5/8Z8ABBYlxO0PRUkrvfQdCZvJKgOWZlzi5fDrevDpskfglMLlBcwfeg1Pvp7n8iSAIbwiaKwwlGuKwhGC2zPiwzMyx3UhjLKwS1yO+4+6o8r+8F4BRgMUET5CSCET+Z+38KfAO4IXx9DwMnFXivniEY/vwv7P38zSPGb0DYpHIRcG/4+mYU2C92PHlcCLydYETPcgZ+Boci1ue3gG8DJ5vZVjO7zN1fBD5AcP7Mi8D/Bj4QHquGkOlhF0mEmTkwyd2fSDoWEVFNQUREIpQUREQkS81HIiKSpZqCiIhkKSmIiEhWqmc2HDNmjE+YMCHpMEREUuWBBx54wd3znt2f6qQwYcIE1qxZk3QYIiKpYma5U3VkqflIRESylBRERCRLSUFERLKUFEREJCvVHc2D0bO2lyUrHmPztl0cPrqVebMm093ZMFOhi4gMSVMlhZ61vVxw63p29QWzFPdu28UFt64HUGIQEaHJmo+WrHgsmxAydvX1s2TFYwlFJCJSX5oqKWzetqus7SIizSaVzUdm1gV0dXR0lPW4w0e30psnARw+urVCkRWn/gwRqXeprCm4+zJ3P7utra30zhHzZk2mdXjLPttah7cwb1ahxbgqJ9Of0bttF87e/oyetYNb2rVnbS8zF69k4vzlzFy8ctDliIhEpTIpDFZ3ZzsXf2Qa7aNbMaB9dCsXf2RaTf5br2R/RqUTjIhIRiqbj4aiu7M9kSabSvZnFEswao4SkaFoqppCkgr1WwymP0Md5iJSLUoKNVLJ/oxKJhgRkSglhRqpZH/GYBKMOqZFJI6m61NIUqX6MzJlxB3eqjO5RSQuJYWUKifBqGNaROJSUmgC5XRM6wQ7keamPoUmELdjWuc/iIiSQhOI2zGtCQNFRM1HTSBux7TOfxARJYUmEadjOukJA0UkeWo+kqwkJwwUkfqgmoJkxW1m0gglkcalpCD7KNXMVOpEOCUMkXRTUpCylBqhpDOnRdJNSUHKUmyEUqmEoRqESP1TUpCyFBuhVChhZGoMuTWINU+/xK//tEWJQqSOaPSRlKXYCKVCQ1dbzPLWIK5bvVFnT4vUGSUFKUuxKcALJYx+97xl5W7V2dMiyTMv8IVNgpl1A38PHApc7u53Ftt/+vTpvmbNmprEJvHkG320ZMVjeZucCmkPm6LUpCRSHWb2gLtPz3df1fsUzOwa4APA8+7+lsj22cC3gRbge+6+2N17gB4zOxj4JlA0KUj9KTSkNdqnAGAMrClktmcSiEYvidReLZqPlgKzoxvMrAW4HDgJmALMMbMpkV0WhPdLA8jX5HT6jPEDmpryJQo1KYnUVtVrCu5+t5lNyNn8TuAJd98AYGY3AB8ys0eBxcAd7v5gtWOT2slXg5h+5CH7NDUVamLShHwitZPUkNR24JnI7U3AscAXgPcBbWbW4e5X5j7QzM4GzgYYP358DUKVaslNFDMXr9SEfCIJS2r0keXZ5u5+mbu/w93PyZcQwp2ucvfp7j597NixVQ5Tainf6KXhw4ydr+1m4vzlzFy8UkNWRaosqZrCJmBc5PYRwOaEYpE6kTshX1vrcHa8tputO/uAoON53s0PceGyR9i2s0+jk0SqIKmkcD8wycwmAr3Ax4HTEopF6ki0SWnm4pVs29W3z/19e3yfJKHRSSKVVYshqdcDxwNjzGwTsNDdv29m5wIrCIakXuPuj5RRZhfQ1dHRUY2QpU7E6WDe1dfPeTeuY8mKx3jvm8Zmp81oax2OGapRiJSprk5eK5dOXmtshTqeB6N1eEv2zGuRZlfs5DVNcyF1K1/H82DpfAeReDRLqtStQh3Pff2Dq93qfAeR0lKZFNSn0Dxyz2XIzK00mGYlne8gUpr6FCSVcpcFLSUzhUa7Op1F1KcgjSfffEpnzBifvT26dTgHjxwO7DunktZtECkulc1HIlB4RtaofCOYMp3Oqi2IDKSagjS0YkuEqrYgMlAqk4KZdZnZVdu3b086FKlzxTqX1YwkMlAqk4K7L3P3s9va2pIORepcsXMddO6CyECpTAoicWU6pAvp3bZLs6+KRGhIqjSFOFNmjBw+jBHDWzRfkjQ8DUmVphdnyoydfXvYurMPR0NXpXmppiBNYyhnQ+ukN2kkxWoKqUwKkWku5j7++ONJhyMpM5TZVw8eOZyFXVOVHCTVGq75SKOPZCiGMvvq1p19alaShpbKpCAyFJkRSaNbhw/q8RrKKo1MSUGaUndnO+sWnsilpx6zz3xJwyze43VGtDQqzX0kTS3f1NxxZ1/V+tDSiFRTEImIzr4KwQyrhagZSRpRKmsKWmRHqilf7eG8G9fl3VeruUmjSWVNQaOPpJa6O9uzNYdcw8zUtyANJZVJQaTWCg1j7XfXEFVpKEoKIjFk+hpabGAvw66+fr562yMJRCVSeak8ozlD01xIrU2cv5w43xid+Sz1rOHOaBZJSrFFe6K27uxj3i0PqVlJUkdJQaQM82ZNjr1vX79ryKqkjpKCSBm6O9s5eGT86TE0ZFXSRklBpEwLu6bGnlDPgc6v3almJEmNVHY0a+psSVrP2l6+etsjbNvVV9bj1AEt9aDh1lPI0OgjSVp04R6DWCOThrcYS05+mxKDJKZYUkjlNBci9SJ3Sow4Q1YzHdBKClKP1KcgUkFxh6yqA1rqlZKCSAXNmzWZ4TEWZXBgwvzl6oSWuqOkIFJB3Z3tLDnlbbFXddu6s4/zblzHgp71VY5MJB71KYhUWLSfoWdtL/980zpKjee4dvVGABZ1T6t2eCJFqaYgUkXdne3xhiQRJAY1J0nSlBREqixu5zMEzUmailuSpKQgUmWF1mIoRFNxS5JSmRTMrMvMrtq+fXvSoYiUlLvucxzbdvWptiCJ0BnNIglY0LOe61Zv1NoMkgitpyBSZxZ1T+Nbpx4Ta+iq1maQWlJSEElId2c76xaeGGsq7r5+Vz+D1ISSgkjCFnZNjbXftl19OslNqq5kUjCzSWZ2i5n9l5ltyFxqEZxIMyhn4Z7rVm9UM5JUVZyawg+AK4DdwHuBHwE/rmZQIs1mYdfU2HMmqRlJqinONBet7v4rMzN3fxr4qpn9FlhY5dhEmkZmZFGchXu27epjwvzlgEYmSeXFSQp/NbNhwONmdi7QCxxa3bBEms+AOZNuXFdyyGpmZFLm8SJDFaf56DxgJPBF4B3AJ4B/qGZQIs2uu7Od02eMj7VvZtEekUooWVNw9/vDq68An6puOCKSsah7Gsv/+Cxbd5ZeB1qL9kilFKwpmNml4d9lZnZb7qV2IYo0r4VdUynd/VzepHsixRSrKWRGGH2zFoGIyEDdne2sefql7HoL+QxvMebNmlzDqKSRlZz7yMxGAbvcfU94uwUY4e47axBfUZr7SJpFz9rekiOTNBJJ4io291GcpLAaeJ+7vxLefh1wp7u/q+KRlklJQZrRgp71RWsOAKP2b+GiD09TgpC8hjoh3gGZhAAQXh9ZqeBEpDzX//6ZkvvseK2fL92sSfSkfHHOU9hhZm939wcBzOwdQKJDHcysC+jq6OhIMgyRRPTHnO6+f49z3o3rOO/GdYCalySeuOcp3Gxmvw3PZL4ROLe6YRXn7svc/ey2trYkwxBJRIvFGY80kKbgljhKJoXwPIU3AZ8FPge82d0fqHZgIpLfnGPHDfqxOtFNSonTfATwN8CEcP9OM8Pdf1S1qESkoEXd0wBir9yWq1cnukkRJZOCmf0YOBpYB/SHm51gtlQRScCi7mnZ5NCztpcLbv0ju/r2xH58z9pe9S1IXnGGpD4KTPE6XMxZQ1JF9upZ28v5N64jbmpQx3PzGuqQ1IeBN1Q2JBGptO7Odi459Rhah8dbUFEdz5JPnJrCr4FjgD8Ar2a2u/sHqxtaaaopiBQ2c/HK2P0HqjU0l2I1hTgdzV+tbDgiUgvzZk1m3s0P0bendMuv1mWQjDhDUn8DPAUMD6/fDzxY5bhEZIi6O9tZcsrbYu+v4aoCMZKCmc0FbgG+G25qB3qqGZSIVEZ3ZztnxFysB4LhqhPmL6fza3eqr6FJxemR+jwwE/gLgLs/jpbjFEmNRd3TykoMoE7oZhYnKbzq7q9lbpjZfjCoc2ZEJCGLuqdx6anHxPrCZ6g5qTnF+Yz8xsz+BWg1sxOAm4Fl1Q1LRCqt3CGrEDQnTZy/nAU966sYmdSTOJ+O+cAWYD3wj8DtwIJqBiUi1dHd2c6jXz+J9jKW73Tg2tUblRiaRJzRR3vc/Wp3P8XdTw6vq/lIJMXmzZpcVlMSxFvHQdIvzuijP5vZhtxLLYITkeoYTFNSvztT//UX6nxucHFOXoue9XYAcApwSHXCEZFa6e5sz56oFvfs58yKbpnHS+OJ03z0YuTS6+6XAn9Xg9hEpEbKaU7q36NRSY0sztTZb4/cHEZQcziwahGJSM1l/uuPOwV35iS3Ufu3cNGHp6nW0EDiNB/9R+T6buDPwMeqE46IJCXanHT0BbfHWgtazUmNp2RScPf31iIQEakfc44dx7WrN8baN9OcpKTQGOI0H/0b8O/uvi28fTDwJXev6LkKZnYU8BWgzd1PrmTZIlKezKpucRNDpjkpY+bRh3Dd3OOqEptUV5y+pZMyCQHA3bcC749TuJldY2bPm9nDOdtnm9ljZvaEmc0Py93g7meVE7yIVM+i7mllneQWde+TL3H61fdVOCKphThJocXMRmRumFkrMKLI/lFLgdnRDWbWAlwOnARMAeaY2ZSY5YlIDc2bNXnQj733yZcqGInUSpykcC3wKzM7y8w+DdwF/DBO4e5+N5D7yXgn8ERYM3gNuAH4UNyAzexsM1tjZmu2bNkS92EiMgjdne1ceuoxlHGOm6RcyeU4IWjuAd4HGHCnu6+I/QRmE4Cfu/tbwtsnA7Pd/TPh7U8AxwILgYuAE4DvufvFpcrWcpwitRV3VFLUiP2G8Y2PvlUd0XVkqMtxAjwK7Hb3X5rZSDM70N1fHmw8eba5u78InDPIMkWkBsoZlZTx6u49nH/TOkDDVtMgiZXXNgHjIrePADYPoTwRqZHBLNgDsMfRWdApEaem8HmCfoDfQ7DympkNZeW1+4FJZjYR6AU+DpxWTgFm1gV0dXR0DCEMERmMRd3TskNWM6LDUQvZHGNuJUleVVdeM7PrgfuAyWa2yczOcvfdwLnACoJmqZvc/ZFygnb3Ze5+dltbWzkPE5EEHT7I4a1SW3FqCrkrr32OmCuvufucAttvJ1isR0QawMyjDyk5BDX3BLczZowfUOOQ5GnlNREZsuvmHsfMo8ubUV+rudWnokNSwxPNfujuZ9QupPg0JFWk/pQzbLXFjCcvjjVBglRQsSGpRWsK7t4PjDWz/asS2SCZWZeZXbV9+/akQxGRHOWcx1DuOQ9SfXH6FJ4C7jWz24AdmY3ufkm1girF3ZcBy6ZPnz43qRhEJL8Ws7JqClJfCtYUzOzH4dVTgZ+H+x4YuYiIDDDn2HGldwr1uzNh/vLsRX0MyStWU3iHmR0JbAT+T43iEZGUK3fa7ajMYzQqKTkFO5rN7IvAZ4GJ7HvGsRFMS3FU9cMrTh3NIukwc/FKemOevKbO5+obVEezu1/m7m8GfuDuR0UuE5NOCOpoFkmXcs5mVudzskqep+Dun61FIOXQGc0i6VLO2czqfE5W3FlSRUQGbd6syZx347pY+2Y6n6MOO3B/fv+VE6oRmuTQ0hkiUnWZxXoG+4Pz3MuvcexFd1U0JslPNQURqYnuzvZ91lMop/MZgsQg1ZfKmoI6mkXST1Np16dUJgV1NIukn6bSrk+pTAoikn7zZk0ua//DDqyrKdgalpKCiCSinM5njT6qnaJTZ9c7ndEs0rhOuGQVjz+/o/SOoUmHjuKu84+vXkANZNBTZ4uIJKHchADw+PM7OOGSVdUJqIkoKYhI3Sk3IQz1cbJXKpOChqSKiFRHKpOChqSKiFRHKpOCiDS2SYeOqunjZC8lBRGpO3edf3zZP/AafVQZmvtIROqSfuCToaQgIqnUs7Y39nTcGapNlKbmIxFJncEkBNC5DHEoKYhI6ixZ8digH6tzGYpLZVLQeQoizU3TbldPKpOCzlMQaW6adrt6UpkURKS5lTvtdpTOZShOSUFEUicz7Xa5NPqoNA1JFZFUyl3zWSpDNQUREclSUhARkSwlBRERyVKfgog0rAU967l29cZBP/6MGeNZ1D2tghHVP9UURKQhDTUhAFy7eiMLetZXKKJ0UFIQkYZ0/e+fqaty0iKVSUHTXIhIKf3udVVOWqQyKWiaCxEppcWsrspJi1QmBRGRUuYcO66uykkLJQURaUiLuqdxxozxQyqjGUcfmae4vWz69Om+Zs2apMMQEUkVM3vA3afnu081BRERyVJSEBGRLCUFERHJUlIQEZEsJQUREclSUhARkSwlBRERydLU2SIiobcu/AV/ebV/yOUc0GL86aL3VyCi2lNNQUSEyiUEgL/2O2/6yu0VKavWlBRERKBiCSHjr/3pnC0ilUlBU2eLiFRHKpOCps4WEamOVCYFEZFKO2hES0XLO6AlneswKCmIiAB/vHB2xRJDmkcfaUiqiEjojxfOTjqExKmmICIiWUoKIiKSpaQgIiJZSgoiIpKlpCAiIllKCiIikqWkICIiWUoKIiKSpaQgIiJZSgoiIpKlpCAiIllKCiIikqWkICIiWUoKIiKSpaQgIiJZSgoiIpKlpCAiIllKCiIiklU3y3Ga2SjgO8BrwCp3vy7hkEREYjnhklU8/vyORJ575tGHcN3c4ypWXlVrCmZ2jZk9b2YP52yfbWaPmdkTZjY/3PwR4BZ3nwt8sJpxiYhUSpIJAeDeJ1/i9Kvvq1h51W4+WgrssxK2mbUAlwMnAVOAOWY2BTgCeCbcrb/KcYmIVESSCSHj3idfqlhZVU0K7n43kBvtO4En3H2Du78G3AB8CNhEkBiKxmVmZ5vZGjNbs2XLlmqELSLStJLoaG5nb40AgmTQDtwKfNTMrgCWFXqwu1/l7tPdffrYsWOrG6mISJNJoqPZ8mxzd98BfKrWwYiIDMWkQ0cl3oQ08+hDKlZWEjWFTcC4yO0jgM0JxCEiMmR3nX88kw4dldjzV3r0URI1hfuBSWY2EegFPg6cVk4BZtYFdHV0dFQhPBGR8tx1/vFJh1Ax1R6Sej1wHzDZzDaZ2Vnuvhs4F1gBPArc5O6PlFOuuy9z97Pb2toqH7SISBOrak3B3ecU2H47cHs1n1tERMqnaS5ERCQrlUnBzLrM7Krt27cnHYqISENJZVJQn4KISHWYuycdw6CZ2Rbg6ZzNbUC+KkTu9jHAC1UKrZRCMdairLj7x9mv2D5xj0OhbY1wfAZTTqWOj45N5ctppGNzpLvnP/vX3RvqAlwVZzuwpt5irEVZcfePs1+xfeIehyLbUn98BlNOpY6Pjo2OzWAvqWw+KqHQFBkFp85IQCVjKbesuPvH2a/YPuUch3o6NlC5eAZTTqWOj45N5ctpimOT6uajoTCzNe4+Pek4JD8dn/qlY1O/KnFsGrGmENdVSQcgRen41C8dm/o15GPTtDUFEREZqJlrCiIikkNJQUREspQUREQkS0khZGajzOyHZna1mZ2edDyyl5kdZWbfN7Nbko5FBjKz7vB78zMzOzHpeGQvM3uzmV1pZreY2WfjPKahk4KZXWNmz5vZwznbZ5vZY2b2hJnNDzd/BLjF3ecCH6x5sE2mnGPjwXreZyUTaXMq8/j0hN+bM4FTEwi3qZR5bB5193OAjwGxhqo2dFIAlgKzoxvMrAW4HDgJmALMMbMpBCvAZdaO7q9hjM1qKfGPjdTeUso/PgvC+6W6llLGsTGzDwL3AL+KU3hDJwV3vxt4KWfzO4Enwv8+XwNuAD5EsEzoEeE+Df2+1IMyj43UWDnHxwLfAO5w9wdrHWuzKfe74+63ufu7gFjN4s3449fO3hoBBMmgHbgV+KiZXUH9ndrfLPIeGzN7vZldCXSa2QXJhCYU/u58AXgfcLKZnZNEYFLwu3O8mV1mZt8l5sJmSazRnDTLs83dfQfwqVoHI/sodGxeBPRjk7xCx+cy4LJaByP7KHRsVgGryimoGWsKm4BxkdtHAJsTikX2pWNT33R86lfFjk0zJoX7gUlmNtHM9gc+DtyWcEwS0LGpbzo+9atix6ahk4KZXQ/cB0w2s01mdpa77wbOBVYAjyQu0E4AAAHnSURBVAI3ufsjScbZjHRs6puOT/2q9rHRhHgiIpLV0DUFEREpj5KCiIhkKSmIiEiWkoKIiGQpKYiISJaSgoiIZCkpiIhIlpKCSBnCGUH1vZGGpQ+3SAlmNsHMHjWz7wAPAt83szVm9oiZXRjZ7ykzu9DMHjSz9Wb2pnD7WDO7K9z+XTN72szGhPedb2YPh5fzknmFInspKYjEMxn4kbt3Al9y9+nAW4H3mNlbI/u94O5vB64A/le4bSGwMtz+U2A8gJm9g2Bm3mOBGcBcM+usyasRKUBJQSSep919dXj9Y2b2ILAWmEqw0lXGreHfB4AJ4fV3Eyx6grv/Atga2f5Td9/h7q+Ej/3bqr0CkRiacT0FkcHYAWBmEwlqAH/j7lvNbClwQGS/V8O//ez9fuWb677YdpHEqKYgUp6DCBLEdjM7jGBN3FLuIVg4HTM7ETg43H430G1mI81sFPBh4LeVD1kkPtUURMrg7g+Z2VrgEWADcG+Mh10IXG9mpwK/AZ4FXnb3B8Oaxh/C/b7n7murELZIbJo6W6TKzGwE0O/uu83sOOAKdz8m6bhE8lFNQaT6xgM3hec3vAbMTTgekYJUUxARkSx1NIuISJaSgoiIZCkpiIhIlpKCiIhkKSmIiEiWkoKIiGT9f/EgEgefGt7jAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "frec = sum([Counter(doc) for doc in train_corpus], Counter())\n",
    "\n",
    "plt.plot(sorted(frec.values(), reverse=True), 'o')\n",
    "plt.xlabel('rango')\n",
    "plt.ylabel('frecuencia')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title('Curva de Zipf de datos de entrenamiento')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sustituyo los hapax por el identificador <unk\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplos:\n",
      "['per', 'pues', 'si', 'el', 'padr', 'del', 'hombr', 'quer', 'lo', 'iba', 'a', 'dej']\n",
      "['hem', 'fabric', 'un', 'temascal', 'par', 'bañ']\n",
      "['ni', 'par', 'el', 'jabon', 'encontr']\n",
      "['mi', 'difunt', 'padrecit', 'fue', 'qui', 'me', 'cur']\n",
      "['se', 'traslad', 'de', 'igual', '<unk>', 'y', 'se', 'le', 'mov', 'el', 'pal']\n"
     ]
    }
   ],
   "source": [
    "train_corpus_unk = [\n",
    "    [\n",
    "        # Si la frecuencia de la palabra es 1, se sustituye por UNK\n",
    "        (UNK if frec[w] == 1 else w)\n",
    "        for w in doc\n",
    "    ] \n",
    "    for doc in train_corpus \n",
    "]\n",
    "\n",
    "print('Ejemplos:')\n",
    "for docs in train_corpus_unk[:5]:\n",
    "        print(docs) # Muestro 5 ejemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 2. <BOS\\> <EOS\\>\n",
    "\n",
    "Insertar símbolos de inicio y final de cadena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<BOS>', 'per', 'pues', 'si', 'el', 'padr', 'del', 'hombr', 'quer', 'lo', 'iba', 'a', 'dej', '<EOS>']\n",
      "['<BOS>', 'hem', 'fabric', 'un', 'temascal', 'par', 'bañ', '<EOS>']\n",
      "['<BOS>', 'ni', 'par', 'el', 'jabon', 'encontr', '<EOS>']\n",
      "['<BOS>', 'mi', 'difunt', 'padrecit', 'fue', 'qui', 'me', 'cur', '<EOS>']\n",
      "['<BOS>', 'se', 'traslad', 'de', 'igual', '<unk>', 'y', 'se', 'le', 'mov', 'el', 'pal', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "train_corpus_unk = [[BOS] + doc + [EOS] for doc in train_corpus_unk]\n",
    "for doc in train_corpus_unk[:5]:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 3. Indexar bigramas\n",
    "\n",
    "Obtener los bigramas que aparecen en el texto (indexar numéricamente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplos de palabras en el vocabulario\n",
      "  <BOS> : 0\n",
      "  <EOS> : 1\n",
      "  per : 2\n",
      "  pues : 3\n",
      "  si : 4\n",
      "  el : 5\n",
      "  padr : 6\n",
      "  del : 7\n",
      "  hombr : 8\n",
      "  quer : 9\n",
      "\n",
      "Ejemplos de oraciones indexadas numéricamente\n",
      "  [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1]\n",
      "  [0, 14, 15, 16, 17, 18, 19, 1]\n",
      "  [0, 20, 18, 5, 21, 22, 1]\n",
      "  [0, 23, 24, 25, 26, 27, 28, 29, 1]\n",
      "  [0, 30, 31, 32, 33, 34, 35, 30, 36, 37, 5, 38, 1]\n",
      "  [0, 39, 40, 41, 42, 18, 43, 1]\n"
     ]
    }
   ],
   "source": [
    "# Indexo numéricamente los stems\n",
    "vocab = defaultdict() \n",
    "# El método default_factory sirve para asignar un nuevo valor por defecto\n",
    "# en caso de no encontrar el indice dado\n",
    "vocab.default_factory = lambda: len(vocab)\n",
    "\n",
    "# Indexo EOS y BOS para que sean los primeros\n",
    "i_BOS = vocab[BOS]\n",
    "i_EOS = vocab[EOS]\n",
    "\n",
    "# Indexo numéricamente las palabras en los documentos\n",
    "corpus_ids = [[vocab[w] for w in doc] for doc in train_corpus_unk]\n",
    "\n",
    "print('Ejemplos de palabras en el vocabulario')\n",
    "for palabra, i in list(vocab.items())[:10]:\n",
    "    print(f'  {palabra} : {i}')\n",
    "\n",
    "print('\\nEjemplos de oraciones indexadas numéricamente')\n",
    "for doc in corpus_ids[:6]:\n",
    "    print(' ', doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculo el inverso del vocabulario, para obtener la palabra dado el índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_vocab = { item : key for key, item in vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```doc[:-1]``` toma todos las cadenas de un documento (frase) excepto la última\n",
    "\n",
    "```doc[1:]``` toma todos las cadenas de un documento (frase) excepto la primera\n",
    "\n",
    "Con zip, uno una cadena de la primer lista con una de la segunda en tuplas (bigramas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<BOS>', 'per'),\n",
       " ('per', 'pues'),\n",
       " ('pues', 'si'),\n",
       " ('si', 'el'),\n",
       " ('el', 'padr'),\n",
       " ('padr', 'del'),\n",
       " ('del', 'hombr'),\n",
       " ('hombr', 'quer'),\n",
       " ('quer', 'lo'),\n",
       " ('lo', 'iba'),\n",
       " ('iba', 'a'),\n",
       " ('a', 'dej'),\n",
       " ('dej', '<EOS>'),\n",
       " ('<BOS>', 'hem'),\n",
       " ('hem', 'fabric'),\n",
       " ('fabric', 'un'),\n",
       " ('un', 'temascal'),\n",
       " ('temascal', 'par'),\n",
       " ('par', 'bañ'),\n",
       " ('bañ', '<EOS>')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigramas = [bi for doc in train_corpus_unk for bi in zip(doc[:-1], doc[1:])]\n",
    "\n",
    "bigramas[:20] # Ejemplos con cadenas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimo algunos de los bigramas más comunes en el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('<unk>', '<EOS>'), 114),\n",
       " (('<BOS>', 'pues'), 88),\n",
       " (('otra', 'vez'), 45),\n",
       " (('<BOS>', 'y'), 42),\n",
       " (('<BOS>', 'per'), 38),\n",
       " (('entonc', '<EOS>'), 36),\n",
       " (('<BOS>', 'se'), 35),\n",
       " (('el', '<unk>'), 29),\n",
       " (('nomas', '<EOS>'), 27),\n",
       " (('se', '<unk>'), 26),\n",
       " (('en', 'la'), 26),\n",
       " (('<BOS>', 'despues'), 25),\n",
       " (('<BOS>', 'no'), 25),\n",
       " (('<BOS>', 'lueg'), 25),\n",
       " (('par', 'que'), 25)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_bigramas = Counter(bigramas)\n",
    "counter_bigramas.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2),\n",
       " (2, 3),\n",
       " (3, 4),\n",
       " (4, 5),\n",
       " (5, 6),\n",
       " (6, 7),\n",
       " (7, 8),\n",
       " (8, 9),\n",
       " (9, 10),\n",
       " (10, 11),\n",
       " (11, 12),\n",
       " (12, 13),\n",
       " (13, 1),\n",
       " (0, 14),\n",
       " (14, 15),\n",
       " (15, 16),\n",
       " (16, 17),\n",
       " (17, 18),\n",
       " (18, 19),\n",
       " (19, 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigramas_ids = [bi for doc in corpus_ids for bi in zip(doc[:-1], doc[1:])]\n",
    "bigramas_ids[:20] # Ejemplos de bigramas indexados numéricamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 4. Modelo Neuronal\n",
    "\n",
    "Entrenar con los bigramas la red neuronal y obtener los valores para los hiperparámetros. Tomar de 100 unidades para la primera capa oculta (capa lineal) y 300 para la segunda capa oculta (capa con tanh)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario:  493\n"
     ]
    }
   ],
   "source": [
    "# Hiperparámetros\n",
    "d = 100# 100\n",
    "m = 300# 300\n",
    "epochs = 25\n",
    "lr = np.exp(-(np.arange(epochs)+1)/2) #0.01\n",
    "\n",
    "# Constantes\n",
    "N = len(vocab)\n",
    "print('Tamaño del vocabulario: ', N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición del modelo\n",
    "\n",
    "Defino la matriz de la primer capa oculta (embedding). $ C \\in \\mathbb{R}^{d \\times N} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bengio2003:\n",
    "    def __init__(self, bigramas, d, m, N):\n",
    "        self.d = d\n",
    "        self.m = m\n",
    "        self.N = N\n",
    "        self.bigramas = bigramas\n",
    "\n",
    "    def inicializar_pesos(self):\n",
    "        self.C = np.random.random((d, N))/np.sqrt(N)\n",
    "        self.W = np.random.random((m, d))/np.sqrt(d)\n",
    "        self.b = np.random.random(m)\n",
    "        self.U = np.random.random((N, m))/np.sqrt(m)\n",
    "        self.c = np.random.random(N)\n",
    "        \n",
    "        # Los mejores pesos del modelo\n",
    "        self.best_C = self.C\n",
    "        self.best_W = self.W\n",
    "        self.best_b = self.b\n",
    "        self.best_U = self.U\n",
    "        self.best_c = self.c\n",
    "    \n",
    "    def forward(self, i_x):\n",
    "        # La capa embedding se reduce a C_1[:, i_x]\n",
    "        h_i = np.tanh(np.dot(self.W, self.C[:, i_x]) + self.b) # Segunda capa oculta\n",
    "        a = self.U.dot(h_i) + self.c # Preactivación\n",
    "        exp_a = np.exp(a - a.max()) # Exponencial de la preactivación\n",
    "        prob_salida = exp_a/exp_a.sum() # Capa de salida softmax\n",
    "        return prob_salida, h_i\n",
    "\n",
    "    def backprop(self, i_x, i_y, prob_salida, h_i, lr=0.1):\n",
    "        y_pred = np.argmax(prob_salida) # El índice de la palabra que predijo\n",
    "        \n",
    "        # Backprop\n",
    "        # Copio el arreglo para no modificar pesos de la salida original\n",
    "        d_out = np.array(prob_salida, copy=True) \n",
    "        d_out[i_y] -= 1  # p(w_k | w_i) - y_k\n",
    "        d_h = (1-h_i**2)*np.dot(d_out.T, self.U)\n",
    "        d_c = np.dot(d_h.T, self.W)\n",
    "\n",
    "        # Actualizamos los pesos\n",
    "        self.U -= lr*np.outer(d_out, h_i) \n",
    "        self.c -= lr*d_out\n",
    "        self.W -= lr*np.outer(d_h, self.C[:,i_x]) \n",
    "        self.b -= lr*d_h\n",
    "        self.C[:, i_x] -= lr*d_c # Las demás filas no nos interesan, porque son 0\n",
    "\n",
    "    def predecir(self, i_x):\n",
    "        return np.argmax(self.forward(i_x)[0])\n",
    "    \n",
    "    def entrenar(self, epochs=50, lr=[]):\n",
    "        entr_timeline = [] # Entropía a través de las épocas\n",
    "        min_entr = np.inf\n",
    "        for epoch in nbtqdm(range(epochs)):\n",
    "            np.random.shuffle(bigramas)\n",
    "            cross_entropy = 0\n",
    "            for bigrama in self.bigramas:\n",
    "                i_x = bigrama[0] # El índice de la primer palabra del bigrama\n",
    "                i_y = bigrama[1] # El índice de la segunda palabra del bigrama\n",
    "                # print(f'  Bigrama: {inv_vocab[i_x]} {inv_vocab[i_y]}')\n",
    "                prob_salida, h_i = self.forward(i_x)\n",
    "                # print(f'  Predicción: {inv_vocab[i_x]} {inv_vocab[np.argmax(prob_salida)]}')\n",
    "                self.backprop(i_x, i_y, prob_salida, h_i, lr[epoch])\n",
    "                cross_entropy -= np.log(prob_salida[i_y])\n",
    "                        \n",
    "            # Si la entropua actual es mejor que la menor...\n",
    "            if cross_entropy < min_entr:\n",
    "                min_entr = cross_entropy  # ponemos la actual\n",
    "                # y movemos los mejores pesos\n",
    "                self.best_C = self.C\n",
    "                self.best_W = self.W\n",
    "                self.best_b = self.b\n",
    "                self.best_U = self.U\n",
    "                self.best_c = self.c\n",
    "                \n",
    "            entr_timeline.append(cross_entropy)    \n",
    "            tqdm.write(f'Epoch: {epoch+1} \\tEntropía cruzada: {cross_entropy}')\n",
    "        return entr_timeline\n",
    "    \n",
    "    def cargar_mejores_pesos(self):\n",
    "        self.C = self.best_C\n",
    "        self.W = self.best_W\n",
    "        self.b = self.best_b\n",
    "        self.U = self.best_U\n",
    "        self.c = self.best_c\n",
    "    \n",
    "    def guardar_pesos(self, archivo):\n",
    "        \"\"\"Guarda los pesos del modelo en formato .npz\n",
    "        \"\"\"\n",
    "        try:\n",
    "            np.savez(\n",
    "                archivo, \n",
    "                C = self.C, \n",
    "                W = self.W, \n",
    "                b = self.b, \n",
    "                U = self.U, \n",
    "                c = self.c\n",
    "            )\n",
    "            print(f'Archivo {archivo} guardado satisfactoriamente')\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print('Ocurrió un error al guardar el archivo')\n",
    "            print(e)\n",
    "            return False\n",
    "    \n",
    "    def cargar_pesos(self, archivo):\n",
    "        \"\"\"Carga los pesos del modelo guardados en un archivo formato .npz\n",
    "        \"\"\"\n",
    "        try:\n",
    "            pesos = np.load(archivo)\n",
    "            self.C = pesos['C']\n",
    "            self.W = pesos['W']\n",
    "            self.b = pesos['b']\n",
    "            self.U = pesos['U']\n",
    "            self.c = pesos['c']\n",
    "            print(f'Pesos desde {archivo} cargados correctamente')\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print('Ocurrió un error al guardar el archivo')\n",
    "            print(e)\n",
    "            return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace47a35a99a45a08b97790e2cff48f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tEntropía cruzada: 1084967.9115285573\n",
      "Epoch: 2 \tEntropía cruzada: 693803.8945785166\n",
      "Epoch: 3 \tEntropía cruzada: 419207.80088428577\n",
      "Epoch: 4 \tEntropía cruzada: 257176.497427378\n",
      "Epoch: 5 \tEntropía cruzada: 161836.38798966436\n",
      "Epoch: 6 \tEntropía cruzada: 105614.64947085583\n",
      "Epoch: 7 \tEntropía cruzada: 72704.60192250437\n",
      "Epoch: 8 \tEntropía cruzada: 55473.969924242454\n",
      "Epoch: 9 \tEntropía cruzada: 47456.24368509424\n",
      "Epoch: 10 \tEntropía cruzada: 43303.060348900515\n",
      "Epoch: 11 \tEntropía cruzada: 41032.71886185665\n",
      "Epoch: 12 \tEntropía cruzada: 39689.13058456585\n",
      "Epoch: 13 \tEntropía cruzada: 38863.35463162758\n",
      "Epoch: 14 \tEntropía cruzada: 38346.672298326965\n",
      "Epoch: 15 \tEntropía cruzada: 38020.565397485436\n",
      "Epoch: 16 \tEntropía cruzada: 37813.93658403837\n",
      "Epoch: 17 \tEntropía cruzada: 37681.95400227908\n",
      "Epoch: 18 \tEntropía cruzada: 37596.822387323555\n",
      "Epoch: 19 \tEntropía cruzada: 37541.87357025355\n",
      "Epoch: 20 \tEntropía cruzada: 37506.800175731805\n",
      "Epoch: 21 \tEntropía cruzada: 37484.5403357785\n",
      "Epoch: 22 \tEntropía cruzada: 37470.37704036314\n",
      "Epoch: 23 \tEntropía cruzada: 37461.40404559118\n",
      "Epoch: 24 \tEntropía cruzada: 37455.784472036285\n",
      "Epoch: 25 \tEntropía cruzada: 37452.30585839929\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelo = Bengio2003(bigramas_ids, d, m, N)\n",
    "modelo.inicializar_pesos()\n",
    "hist_entropia = modelo.entrenar(epochs, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardando pesos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo modelos/pesos_3.npz guardado satisfactoriamente\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.cargar_mejores_pesos()\n",
    "\n",
    "np.savez(\n",
    "    'modelos/hiperp_3.npz', \n",
    "    d = d,\n",
    "    m = m,\n",
    "    lr = lr,\n",
    "    epochs = epochs,\n",
    "    SEED = SEED,\n",
    "    N = N\n",
    ")\n",
    "\n",
    "modelo.guardar_pesos('modelos/pesos_3.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 5. Obtención de matrices de probabilidades\n",
    "\n",
    "Obtener las matrices $A$ y $\\Pi$ a partir de las salidas de la red neuornal (probabilidad Softmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>per</th>\n",
       "      <th>pues</th>\n",
       "      <th>si</th>\n",
       "      <th>el</th>\n",
       "      <th>padr</th>\n",
       "      <th>del</th>\n",
       "      <th>hombr</th>\n",
       "      <th>quer</th>\n",
       "      <th>lo</th>\n",
       "      <th>iba</th>\n",
       "      <th>...</th>\n",
       "      <th>grit</th>\n",
       "      <th>siqu</th>\n",
       "      <th>maicit</th>\n",
       "      <th>negoci</th>\n",
       "      <th>culebr</th>\n",
       "      <th>grandot</th>\n",
       "      <th>docen</th>\n",
       "      <th>cre</th>\n",
       "      <th>persegu</th>\n",
       "      <th>alzad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;EOS&gt;</th>\n",
       "      <td>6.095120e-11</td>\n",
       "      <td>2.880933e-09</td>\n",
       "      <td>3.577234e-01</td>\n",
       "      <td>3.167132e-07</td>\n",
       "      <td>1.298466e-03</td>\n",
       "      <td>7.469338e-02</td>\n",
       "      <td>3.028953e-01</td>\n",
       "      <td>7.469338e-02</td>\n",
       "      <td>1.709970e-01</td>\n",
       "      <td>7.469338e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>4.213193e-10</td>\n",
       "      <td>1.875562e-10</td>\n",
       "      <td>7.978878e-01</td>\n",
       "      <td>9.885440e-01</td>\n",
       "      <td>1.272617e-13</td>\n",
       "      <td>5.413066e-01</td>\n",
       "      <td>3.577234e-01</td>\n",
       "      <td>9.601790e-04</td>\n",
       "      <td>3.924648e-07</td>\n",
       "      <td>4.003724e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>per</th>\n",
       "      <td>1.409033e-16</td>\n",
       "      <td>1.570594e-23</td>\n",
       "      <td>1.262038e-11</td>\n",
       "      <td>3.195138e-11</td>\n",
       "      <td>1.936303e-12</td>\n",
       "      <td>2.682917e-07</td>\n",
       "      <td>1.002261e-09</td>\n",
       "      <td>2.682917e-07</td>\n",
       "      <td>4.292788e-03</td>\n",
       "      <td>2.682917e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>3.159301e-12</td>\n",
       "      <td>3.495594e-21</td>\n",
       "      <td>7.741706e-15</td>\n",
       "      <td>5.506754e-06</td>\n",
       "      <td>5.424275e-11</td>\n",
       "      <td>1.317282e-11</td>\n",
       "      <td>1.262038e-11</td>\n",
       "      <td>2.265815e-14</td>\n",
       "      <td>2.061713e-18</td>\n",
       "      <td>2.112122e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pues</th>\n",
       "      <td>2.092880e-01</td>\n",
       "      <td>4.285074e-21</td>\n",
       "      <td>4.142400e-05</td>\n",
       "      <td>5.621389e-15</td>\n",
       "      <td>3.007979e-05</td>\n",
       "      <td>1.593456e-04</td>\n",
       "      <td>1.598703e-09</td>\n",
       "      <td>1.593456e-04</td>\n",
       "      <td>2.983766e-03</td>\n",
       "      <td>1.593456e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>7.470174e-13</td>\n",
       "      <td>8.005205e-01</td>\n",
       "      <td>1.542855e-07</td>\n",
       "      <td>4.742446e-10</td>\n",
       "      <td>4.488121e-15</td>\n",
       "      <td>9.169372e-04</td>\n",
       "      <td>4.142400e-05</td>\n",
       "      <td>4.451103e-14</td>\n",
       "      <td>8.925645e-04</td>\n",
       "      <td>5.055143e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>si</th>\n",
       "      <td>3.531344e-02</td>\n",
       "      <td>1.549003e-02</td>\n",
       "      <td>2.855011e-07</td>\n",
       "      <td>5.584979e-11</td>\n",
       "      <td>8.530603e-15</td>\n",
       "      <td>7.125480e-04</td>\n",
       "      <td>2.734899e-08</td>\n",
       "      <td>7.125480e-04</td>\n",
       "      <td>1.351273e-03</td>\n",
       "      <td>7.125480e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>1.117093e-12</td>\n",
       "      <td>5.712231e-07</td>\n",
       "      <td>1.808123e-11</td>\n",
       "      <td>5.467829e-13</td>\n",
       "      <td>9.681056e-13</td>\n",
       "      <td>5.314911e-12</td>\n",
       "      <td>2.855011e-07</td>\n",
       "      <td>9.311220e-08</td>\n",
       "      <td>7.836577e-06</td>\n",
       "      <td>3.003458e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>el</th>\n",
       "      <td>1.938992e-04</td>\n",
       "      <td>2.379912e-02</td>\n",
       "      <td>8.477417e-03</td>\n",
       "      <td>8.458677e-09</td>\n",
       "      <td>1.013812e-05</td>\n",
       "      <td>9.073771e-04</td>\n",
       "      <td>3.290136e-13</td>\n",
       "      <td>9.073771e-04</td>\n",
       "      <td>3.049276e-04</td>\n",
       "      <td>9.073771e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>8.347387e-09</td>\n",
       "      <td>1.116591e-06</td>\n",
       "      <td>1.312908e-07</td>\n",
       "      <td>2.210594e-06</td>\n",
       "      <td>3.941201e-02</td>\n",
       "      <td>4.980735e-03</td>\n",
       "      <td>8.477417e-03</td>\n",
       "      <td>2.037200e-09</td>\n",
       "      <td>3.503660e-11</td>\n",
       "      <td>8.752222e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grandot</th>\n",
       "      <td>2.119146e-12</td>\n",
       "      <td>1.108908e-15</td>\n",
       "      <td>8.084118e-08</td>\n",
       "      <td>2.060505e-10</td>\n",
       "      <td>2.336502e-15</td>\n",
       "      <td>4.979038e-08</td>\n",
       "      <td>1.418389e-03</td>\n",
       "      <td>4.979038e-08</td>\n",
       "      <td>2.278419e-04</td>\n",
       "      <td>4.979038e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>9.724083e-16</td>\n",
       "      <td>2.230608e-18</td>\n",
       "      <td>6.098235e-06</td>\n",
       "      <td>9.610348e-13</td>\n",
       "      <td>7.811351e-19</td>\n",
       "      <td>7.695965e-06</td>\n",
       "      <td>8.084118e-08</td>\n",
       "      <td>2.160671e-18</td>\n",
       "      <td>3.110625e-08</td>\n",
       "      <td>3.216933e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>docen</th>\n",
       "      <td>6.398611e-12</td>\n",
       "      <td>7.472199e-07</td>\n",
       "      <td>3.032560e-03</td>\n",
       "      <td>3.571545e-12</td>\n",
       "      <td>2.803088e-13</td>\n",
       "      <td>1.157837e-09</td>\n",
       "      <td>1.199610e-11</td>\n",
       "      <td>1.157837e-09</td>\n",
       "      <td>3.470473e-08</td>\n",
       "      <td>1.157837e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>5.351854e-15</td>\n",
       "      <td>6.911231e-11</td>\n",
       "      <td>9.399328e-16</td>\n",
       "      <td>1.157647e-14</td>\n",
       "      <td>1.774804e-08</td>\n",
       "      <td>6.431133e-15</td>\n",
       "      <td>3.032560e-03</td>\n",
       "      <td>8.323092e-11</td>\n",
       "      <td>5.005086e-06</td>\n",
       "      <td>3.113761e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cre</th>\n",
       "      <td>5.214122e-02</td>\n",
       "      <td>6.956105e-14</td>\n",
       "      <td>2.195537e-03</td>\n",
       "      <td>1.135165e-03</td>\n",
       "      <td>2.123306e-10</td>\n",
       "      <td>9.643221e-09</td>\n",
       "      <td>6.272817e-12</td>\n",
       "      <td>9.643221e-09</td>\n",
       "      <td>7.098140e-08</td>\n",
       "      <td>9.643221e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>2.516156e-14</td>\n",
       "      <td>1.182089e-11</td>\n",
       "      <td>8.675073e-14</td>\n",
       "      <td>7.312773e-09</td>\n",
       "      <td>2.944116e-10</td>\n",
       "      <td>5.529620e-14</td>\n",
       "      <td>2.195537e-03</td>\n",
       "      <td>1.452163e-10</td>\n",
       "      <td>2.824734e-11</td>\n",
       "      <td>5.813002e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>persegu</th>\n",
       "      <td>3.918725e-11</td>\n",
       "      <td>3.011148e-06</td>\n",
       "      <td>1.065102e-05</td>\n",
       "      <td>4.988335e-08</td>\n",
       "      <td>1.181462e-06</td>\n",
       "      <td>5.656405e-08</td>\n",
       "      <td>1.438284e-16</td>\n",
       "      <td>5.656405e-08</td>\n",
       "      <td>2.384018e-06</td>\n",
       "      <td>5.656405e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>4.481759e-13</td>\n",
       "      <td>3.153350e-10</td>\n",
       "      <td>5.549489e-18</td>\n",
       "      <td>1.932020e-07</td>\n",
       "      <td>5.891442e-07</td>\n",
       "      <td>1.307412e-17</td>\n",
       "      <td>1.065102e-05</td>\n",
       "      <td>4.805903e-15</td>\n",
       "      <td>1.536117e-15</td>\n",
       "      <td>1.036444e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alzad</th>\n",
       "      <td>1.193023e-13</td>\n",
       "      <td>5.511821e-15</td>\n",
       "      <td>3.601289e-07</td>\n",
       "      <td>8.675979e-12</td>\n",
       "      <td>1.450732e-08</td>\n",
       "      <td>1.012895e-05</td>\n",
       "      <td>2.966870e-17</td>\n",
       "      <td>1.012895e-05</td>\n",
       "      <td>2.432369e-04</td>\n",
       "      <td>1.012895e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>4.488276e-17</td>\n",
       "      <td>4.489292e-16</td>\n",
       "      <td>1.388526e-16</td>\n",
       "      <td>3.278056e-11</td>\n",
       "      <td>6.355477e-05</td>\n",
       "      <td>1.369977e-15</td>\n",
       "      <td>3.601289e-07</td>\n",
       "      <td>3.447461e-10</td>\n",
       "      <td>1.485336e-09</td>\n",
       "      <td>1.502802e-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>492 rows × 491 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  per          pues            si            el          padr  \\\n",
       "<EOS>    6.095120e-11  2.880933e-09  3.577234e-01  3.167132e-07  1.298466e-03   \n",
       "per      1.409033e-16  1.570594e-23  1.262038e-11  3.195138e-11  1.936303e-12   \n",
       "pues     2.092880e-01  4.285074e-21  4.142400e-05  5.621389e-15  3.007979e-05   \n",
       "si       3.531344e-02  1.549003e-02  2.855011e-07  5.584979e-11  8.530603e-15   \n",
       "el       1.938992e-04  2.379912e-02  8.477417e-03  8.458677e-09  1.013812e-05   \n",
       "...               ...           ...           ...           ...           ...   \n",
       "grandot  2.119146e-12  1.108908e-15  8.084118e-08  2.060505e-10  2.336502e-15   \n",
       "docen    6.398611e-12  7.472199e-07  3.032560e-03  3.571545e-12  2.803088e-13   \n",
       "cre      5.214122e-02  6.956105e-14  2.195537e-03  1.135165e-03  2.123306e-10   \n",
       "persegu  3.918725e-11  3.011148e-06  1.065102e-05  4.988335e-08  1.181462e-06   \n",
       "alzad    1.193023e-13  5.511821e-15  3.601289e-07  8.675979e-12  1.450732e-08   \n",
       "\n",
       "                  del         hombr          quer            lo           iba  \\\n",
       "<EOS>    7.469338e-02  3.028953e-01  7.469338e-02  1.709970e-01  7.469338e-02   \n",
       "per      2.682917e-07  1.002261e-09  2.682917e-07  4.292788e-03  2.682917e-07   \n",
       "pues     1.593456e-04  1.598703e-09  1.593456e-04  2.983766e-03  1.593456e-04   \n",
       "si       7.125480e-04  2.734899e-08  7.125480e-04  1.351273e-03  7.125480e-04   \n",
       "el       9.073771e-04  3.290136e-13  9.073771e-04  3.049276e-04  9.073771e-04   \n",
       "...               ...           ...           ...           ...           ...   \n",
       "grandot  4.979038e-08  1.418389e-03  4.979038e-08  2.278419e-04  4.979038e-08   \n",
       "docen    1.157837e-09  1.199610e-11  1.157837e-09  3.470473e-08  1.157837e-09   \n",
       "cre      9.643221e-09  6.272817e-12  9.643221e-09  7.098140e-08  9.643221e-09   \n",
       "persegu  5.656405e-08  1.438284e-16  5.656405e-08  2.384018e-06  5.656405e-08   \n",
       "alzad    1.012895e-05  2.966870e-17  1.012895e-05  2.432369e-04  1.012895e-05   \n",
       "\n",
       "         ...          grit          siqu        maicit        negoci  \\\n",
       "<EOS>    ...  4.213193e-10  1.875562e-10  7.978878e-01  9.885440e-01   \n",
       "per      ...  3.159301e-12  3.495594e-21  7.741706e-15  5.506754e-06   \n",
       "pues     ...  7.470174e-13  8.005205e-01  1.542855e-07  4.742446e-10   \n",
       "si       ...  1.117093e-12  5.712231e-07  1.808123e-11  5.467829e-13   \n",
       "el       ...  8.347387e-09  1.116591e-06  1.312908e-07  2.210594e-06   \n",
       "...      ...           ...           ...           ...           ...   \n",
       "grandot  ...  9.724083e-16  2.230608e-18  6.098235e-06  9.610348e-13   \n",
       "docen    ...  5.351854e-15  6.911231e-11  9.399328e-16  1.157647e-14   \n",
       "cre      ...  2.516156e-14  1.182089e-11  8.675073e-14  7.312773e-09   \n",
       "persegu  ...  4.481759e-13  3.153350e-10  5.549489e-18  1.932020e-07   \n",
       "alzad    ...  4.488276e-17  4.489292e-16  1.388526e-16  3.278056e-11   \n",
       "\n",
       "               culebr       grandot         docen           cre       persegu  \\\n",
       "<EOS>    1.272617e-13  5.413066e-01  3.577234e-01  9.601790e-04  3.924648e-07   \n",
       "per      5.424275e-11  1.317282e-11  1.262038e-11  2.265815e-14  2.061713e-18   \n",
       "pues     4.488121e-15  9.169372e-04  4.142400e-05  4.451103e-14  8.925645e-04   \n",
       "si       9.681056e-13  5.314911e-12  2.855011e-07  9.311220e-08  7.836577e-06   \n",
       "el       3.941201e-02  4.980735e-03  8.477417e-03  2.037200e-09  3.503660e-11   \n",
       "...               ...           ...           ...           ...           ...   \n",
       "grandot  7.811351e-19  7.695965e-06  8.084118e-08  2.160671e-18  3.110625e-08   \n",
       "docen    1.774804e-08  6.431133e-15  3.032560e-03  8.323092e-11  5.005086e-06   \n",
       "cre      2.944116e-10  5.529620e-14  2.195537e-03  1.452163e-10  2.824734e-11   \n",
       "persegu  5.891442e-07  1.307412e-17  1.065102e-05  4.805903e-15  1.536117e-15   \n",
       "alzad    6.355477e-05  1.369977e-15  3.601289e-07  3.447461e-10  1.485336e-09   \n",
       "\n",
       "                alzad  \n",
       "<EOS>    4.003724e-09  \n",
       "per      2.112122e-06  \n",
       "pues     5.055143e-08  \n",
       "si       3.003458e-11  \n",
       "el       8.752222e-01  \n",
       "...               ...  \n",
       "grandot  3.216933e-06  \n",
       "docen    3.113761e-12  \n",
       "cre      5.813002e-14  \n",
       "persegu  1.036444e-17  \n",
       "alzad    1.502802e-13  \n",
       "\n",
       "[492 rows x 491 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;BOS&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>per</th>\n",
       "      <td>5.249771e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pues</th>\n",
       "      <td>1.163569e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>si</th>\n",
       "      <td>1.942594e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>el</th>\n",
       "      <td>2.119252e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>padr</th>\n",
       "      <td>1.520437e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grandot</th>\n",
       "      <td>2.856925e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>docen</th>\n",
       "      <td>1.202006e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cre</th>\n",
       "      <td>1.292094e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>persegu</th>\n",
       "      <td>5.428261e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alzad</th>\n",
       "      <td>6.527891e-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>491 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                <BOS>\n",
       "per      5.249771e-02\n",
       "pues     1.163569e-01\n",
       "si       1.942594e-02\n",
       "el       2.119252e-02\n",
       "padr     1.520437e-12\n",
       "...               ...\n",
       "grandot  2.856925e-14\n",
       "docen    1.202006e-06\n",
       "cre      1.292094e-07\n",
       "persegu  5.428261e-06\n",
       "alzad    6.527891e-12\n",
       "\n",
       "[491 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i_BOS = vocab[BOS]\n",
    "i_EOS = vocab[EOS]\n",
    "\n",
    "matA = np.array([\n",
    "    modelo.forward(i_x)[0] # Sólo nos interesa el vector de probabilidades\n",
    "    for i_x in range(N)  # Iterar sobre todas las palabras del vocabulario\n",
    "])\n",
    "matPi = modelo.forward(vocab[BOS])[0]\n",
    "\n",
    "matA = np.delete(matA, [i_BOS, i_EOS], 0) # Elimino las columnas p(w_j | w_i = <BOS>) y p(w_j | w_i = <EOS>)\n",
    "labelsColsA = list(vocab.keys())[:N] \n",
    "del labelsColsA[i_BOS]\n",
    "del labelsColsA[i_EOS-1]\n",
    "matA = np.delete(matA, i_BOS, 1) # Elimino el valor p(w_j = <BOS> | w_i)\n",
    "labelsRowsA = list(vocab.keys())[:N] \n",
    "del labelsRowsA[i_BOS]\n",
    "\n",
    "matPi = np.delete(matPi, [i_BOS, i_EOS]) # Elimino los valores p(w_j | w_i = <BOS>) y p(w_j | w_i = <EOS>) \n",
    "labelsPi = list(vocab.keys())[:N] \n",
    "del labelsPi[i_BOS]\n",
    "del labelsPi[i_EOS-1]\n",
    "\n",
    "matA = pd.DataFrame(matA, index=labelsColsA, columns=labelsRowsA)\n",
    "display(matA.T)\n",
    "\n",
    "matPi = pd.DataFrame(matPi, index=labelsPi, columns=[BOS])\n",
    "display(matPi)\n",
    "# print('Dimensiones de A: ', matA.shape)\n",
    "# print('Dimensiones de Pi: ', matPi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 6. Evaluación del modelo\n",
    "\n",
    "Evaluar el modelo (con entropía)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist_entropia)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicción de algunos bigramas en el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción de bigramas:\n",
      "  pues vez\n",
      "  por vez\n",
      "  par vez\n",
      "  en vez\n",
      "  otra vez\n"
     ]
    }
   ],
   "source": [
    "test_w = ['pues', 'por', 'par', 'en', 'otra']\n",
    "print('Predicción de bigramas:')\n",
    "for w in test_w:\n",
    "    print(f'  {w} {inv_vocab[modelo.predecir(i_w)]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 7. Probabilidad de oraciones.\n",
    "\n",
    "Calcular la probabilidad de las siguientes oraciones:\n",
    "\n",
    "- Nos bañamos con agua caliente\n",
    "- El animalito le olía la cabeza\n",
    "- Pascuala ordeñaba las vacas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nos', 'bañ', 'con', 'agu', 'calient'],\n",
       " ['el', 'animalit', 'le', 'oli', 'la', 'cabez'],\n",
       " ['pascual', 'ordeñ', 'las', 'vac'],\n",
       " ['vac', 'la', 'per', 'com', 'dos']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oraciones_test = [\n",
    "    \"Nos bañamos con agua caliente\",\n",
    "    \"El animalito le olía la cabeza\",\n",
    "    \"Pascuala ordeñaba las vacas\",\n",
    "    \"Vaca la pero come dos\"\n",
    "]\n",
    "test_stems = [\n",
    "        list(map(lambda token: stemmer_esp.stem(token), # Mapeo cada token a su stem\n",
    "            re.findall('[a-zA-zñáéíóúü]+', oracion.lower()) # Devuelve una lista con todas las ocurrencias que coincidan con la regex\n",
    "        ))\n",
    "        for oracion in oraciones_test\n",
    "    ]\n",
    "\n",
    "test_stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<BOS>', 'nos', 'bañ', 'con', 'agu', 'calient', '<EOS>'],\n",
       " ['<BOS>', 'el', 'animalit', 'le', 'oli', 'la', 'cabez', '<EOS>'],\n",
       " ['<BOS>', 'pascual', 'ordeñ', 'las', 'vac', '<EOS>'],\n",
       " ['<BOS>', 'vac', 'la', 'per', 'com', 'dos', '<EOS>']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agrego BOS y EOS\n",
    "test_stems = [[BOS, *oracion, EOS] for oracion in test_stems]\n",
    "test_stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexo las oraciones de prueba. Si la palabra queda fuera del vocabulario, le asigno el identificador <unk\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 93, 19, 80, 390, 34, 1],\n",
       " [0, 5, 465, 36, 34, 65, 311, 1],\n",
       " [0, 386, 34, 41, 207, 1],\n",
       " [0, 207, 65, 2, 56, 114, 1]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_UNK = vocab[UNK]\n",
    "test_corpus_ids = [\n",
    "    [\n",
    "        vocab[stem] if vocab[stem]<N else i_UNK # Si la palabra es nueva, uso el indice de <unk>\n",
    "        for stem in oracion\n",
    "    ]\n",
    "    for oracion in test_stems\n",
    "]\n",
    "test_corpus_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 93), (93, 19), (19, 80), (80, 390), (390, 34), (34, 1)],\n",
       " [(0, 5), (5, 465), (465, 36), (36, 34), (34, 65), (65, 311), (311, 1)],\n",
       " [(0, 386), (386, 34), (34, 41), (41, 207), (207, 1)],\n",
       " [(0, 207), (207, 65), (65, 2), (2, 56), (56, 114), (114, 1)]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saco los bigramas\n",
    "test_bigramas = [\n",
    "    [bi for bi in zip(oracion[:-1], oracion[1:])]\n",
    "    for oracion in test_corpus_ids\n",
    "]\n",
    "test_bigramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculo las probabilidades de las oraciones.\n",
    "\n",
    "$$ p(w_1) \\prod_{i=1} p(w_{i} | w_{i-1}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_lista_bigramas(lista_bi, A, Pi):\n",
    "    prob = 1.0\n",
    "    for w_i, w_j in lista_bi:\n",
    "        prob *= modelo.forward(w_i)[0][w_j]\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidad de \"Nos bañamos con agua caliente\" = 6.888483633973914e-21\n",
      "Probabilidad de \"El animalito le olía la cabeza\" = 2.6684510596950435e-24\n",
      "Probabilidad de \"Pascuala ordeñaba las vacas\" = 2.0989383626948116e-23\n",
      "Probabilidad de \"Vaca la pero come dos\" = 5.434131460399946e-37\n"
     ]
    }
   ],
   "source": [
    "for lista_bi, oracion in zip(test_bigramas, oraciones_test):\n",
    "    print(f'Probabilidad de \\\"{oracion}\\\" = {prob_lista_bigramas(lista_bi, matA, matPi)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
